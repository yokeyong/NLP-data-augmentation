{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from embedding import get_corpus\n",
    "from nltk import pos_tag\n",
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from textgenrnn import textgenrnn\n",
    "#from augment import csv_to_txt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google corpus\n"
     ]
    }
   ],
   "source": [
    "#load in corpus\n",
    "model = get_corpus('google')\n",
    "print('Loaded google corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create appropriate dataframes for output\n",
    "x_col = 'tweet'\n",
    "y_col = 'class'\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "augmented = pd.DataFrame(columns=[x_col, y_col])\n",
    "#get number of classes in target file\n",
    "no_classes = df.apply(lambda x: x.nunique())['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define what class to augment\n",
    "n = 0\n",
    "# context-specific dictionary\n",
    "classes = { 'hate': 0, \n",
    "            1: 'offensive', \n",
    "            2: 'neither'\n",
    "}\n",
    "# class-filtered dataframe\n",
    "df_class = df[df[y_col] == n]\n",
    "class_path = 'class_filter.csv'\n",
    "df_class.to_csv(class_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,430 texts collected.\n",
      "Training new model w/ 2-layer, 128-cell LSTMs\n",
      "Training on 51,050 word sequences.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/398 [======================>.......] - ETA: 16:15 - loss: 8.77 - ETA: 9:36 - loss: 8.4795 - ETA: 7:19 - loss: 8.602 - ETA: 6:14 - loss: 8.507 - ETA: 5:31 - loss: 8.657 - ETA: 5:04 - loss: 8.144 - ETA: 4:43 - loss: 7.839 - ETA: 4:28 - loss: 7.604 - ETA: 4:16 - loss: 7.393 - ETA: 4:06 - loss: 7.285 - ETA: 3:58 - loss: 7.144 - ETA: 3:51 - loss: 6.991 - ETA: 3:45 - loss: 6.880 - ETA: 3:40 - loss: 6.793 - ETA: 3:36 - loss: 6.683 - ETA: 3:32 - loss: 6.583 - ETA: 3:28 - loss: 6.528 - ETA: 3:26 - loss: 6.427 - ETA: 3:23 - loss: 6.361 - ETA: 3:20 - loss: 6.282 - ETA: 3:18 - loss: 6.238 - ETA: 3:16 - loss: 6.191 - ETA: 3:14 - loss: 6.159 - ETA: 3:13 - loss: 6.109 - ETA: 3:12 - loss: 6.066 - ETA: 3:10 - loss: 6.014 - ETA: 3:09 - loss: 5.989 - ETA: 3:07 - loss: 5.983 - ETA: 3:06 - loss: 5.958 - ETA: 3:05 - loss: 5.896 - ETA: 3:05 - loss: 5.861 - ETA: 3:04 - loss: 5.838 - ETA: 3:04 - loss: 5.823 - ETA: 3:03 - loss: 5.808 - ETA: 3:01 - loss: 5.786 - ETA: 3:00 - loss: 5.757 - ETA: 2:59 - loss: 5.735 - ETA: 2:58 - loss: 5.708 - ETA: 2:58 - loss: 5.688 - ETA: 2:57 - loss: 5.670 - ETA: 2:56 - loss: 5.668 - ETA: 2:55 - loss: 5.653 - ETA: 2:54 - loss: 5.650 - ETA: 2:53 - loss: 5.648 - ETA: 2:52 - loss: 5.628 - ETA: 2:51 - loss: 5.615 - ETA: 2:51 - loss: 5.600 - ETA: 2:50 - loss: 5.584 - ETA: 2:49 - loss: 5.573 - ETA: 2:49 - loss: 5.561 - ETA: 2:48 - loss: 5.547 - ETA: 2:48 - loss: 5.537 - ETA: 2:47 - loss: 5.530 - ETA: 2:46 - loss: 5.523 - ETA: 2:45 - loss: 5.507 - ETA: 2:45 - loss: 5.498 - ETA: 2:44 - loss: 5.491 - ETA: 2:43 - loss: 5.473 - ETA: 2:43 - loss: 5.472 - ETA: 2:42 - loss: 5.464 - ETA: 2:41 - loss: 5.461 - ETA: 2:41 - loss: 5.443 - ETA: 2:40 - loss: 5.436 - ETA: 2:39 - loss: 5.435 - ETA: 2:39 - loss: 5.413 - ETA: 2:38 - loss: 5.403 - ETA: 2:37 - loss: 5.389 - ETA: 2:37 - loss: 5.377 - ETA: 2:36 - loss: 5.372 - ETA: 2:36 - loss: 5.370 - ETA: 2:35 - loss: 5.358 - ETA: 2:35 - loss: 5.351 - ETA: 2:34 - loss: 5.339 - ETA: 2:34 - loss: 5.334 - ETA: 2:34 - loss: 5.325 - ETA: 2:33 - loss: 5.318 - ETA: 2:32 - loss: 5.307 - ETA: 2:32 - loss: 5.298 - ETA: 2:31 - loss: 5.292 - ETA: 2:31 - loss: 5.283 - ETA: 2:30 - loss: 5.269 - ETA: 2:30 - loss: 5.261 - ETA: 2:29 - loss: 5.258 - ETA: 2:29 - loss: 5.251 - ETA: 2:28 - loss: 5.243 - ETA: 2:27 - loss: 5.235 - ETA: 2:27 - loss: 5.232 - ETA: 2:26 - loss: 5.227 - ETA: 2:26 - loss: 5.222 - ETA: 2:25 - loss: 5.210 - ETA: 2:25 - loss: 5.204 - ETA: 2:25 - loss: 5.198 - ETA: 2:24 - loss: 5.190 - ETA: 2:24 - loss: 5.182 - ETA: 2:23 - loss: 5.181 - ETA: 2:22 - loss: 5.173 - ETA: 2:22 - loss: 5.167 - ETA: 2:21 - loss: 5.158 - ETA: 2:21 - loss: 5.153 - ETA: 2:20 - loss: 5.149 - ETA: 2:20 - loss: 5.145 - ETA: 2:20 - loss: 5.146 - ETA: 2:19 - loss: 5.139 - ETA: 2:19 - loss: 5.136 - ETA: 2:18 - loss: 5.130 - ETA: 2:18 - loss: 5.125 - ETA: 2:17 - loss: 5.118 - ETA: 2:17 - loss: 5.115 - ETA: 2:16 - loss: 5.108 - ETA: 2:16 - loss: 5.097 - ETA: 2:15 - loss: 5.089 - ETA: 2:15 - loss: 5.088 - ETA: 2:14 - loss: 5.088 - ETA: 2:14 - loss: 5.082 - ETA: 2:13 - loss: 5.076 - ETA: 2:13 - loss: 5.067 - ETA: 2:12 - loss: 5.065 - ETA: 2:12 - loss: 5.057 - ETA: 2:11 - loss: 5.053 - ETA: 2:11 - loss: 5.050 - ETA: 2:10 - loss: 5.047 - ETA: 2:10 - loss: 5.041 - ETA: 2:09 - loss: 5.034 - ETA: 2:09 - loss: 5.029 - ETA: 2:08 - loss: 5.024 - ETA: 2:08 - loss: 5.019 - ETA: 2:07 - loss: 5.016 - ETA: 2:07 - loss: 5.004 - ETA: 2:07 - loss: 4.997 - ETA: 2:06 - loss: 4.997 - ETA: 2:06 - loss: 4.995 - ETA: 2:05 - loss: 4.990 - ETA: 2:05 - loss: 4.983 - ETA: 2:05 - loss: 4.976 - ETA: 2:04 - loss: 4.972 - ETA: 2:04 - loss: 4.970 - ETA: 2:03 - loss: 4.965 - ETA: 2:03 - loss: 4.958 - ETA: 2:02 - loss: 4.954 - ETA: 2:02 - loss: 4.952 - ETA: 2:02 - loss: 4.947 - ETA: 2:01 - loss: 4.945 - ETA: 2:00 - loss: 4.947 - ETA: 2:00 - loss: 4.944 - ETA: 1:59 - loss: 4.939 - ETA: 1:59 - loss: 4.934 - ETA: 1:59 - loss: 4.932 - ETA: 1:58 - loss: 4.927 - ETA: 1:58 - loss: 4.922 - ETA: 1:57 - loss: 4.917 - ETA: 1:56 - loss: 4.915 - ETA: 1:56 - loss: 4.908 - ETA: 1:56 - loss: 4.905 - ETA: 1:55 - loss: 4.903 - ETA: 1:55 - loss: 4.903 - ETA: 1:54 - loss: 4.904 - ETA: 1:54 - loss: 4.903 - ETA: 1:53 - loss: 4.898 - ETA: 1:53 - loss: 4.900 - ETA: 1:52 - loss: 4.897 - ETA: 1:52 - loss: 4.890 - ETA: 1:51 - loss: 4.888 - ETA: 1:51 - loss: 4.887 - ETA: 1:50 - loss: 4.885 - ETA: 1:50 - loss: 4.881 - ETA: 1:49 - loss: 4.877 - ETA: 1:49 - loss: 4.871 - ETA: 1:48 - loss: 4.868 - ETA: 1:48 - loss: 4.863 - ETA: 1:47 - loss: 4.859 - ETA: 1:47 - loss: 4.854 - ETA: 1:46 - loss: 4.852 - ETA: 1:46 - loss: 4.846 - ETA: 1:46 - loss: 4.843 - ETA: 1:45 - loss: 4.842 - ETA: 1:45 - loss: 4.840 - ETA: 1:44 - loss: 4.837 - ETA: 1:44 - loss: 4.832 - ETA: 1:43 - loss: 4.829 - ETA: 1:43 - loss: 4.826 - ETA: 1:42 - loss: 4.821 - ETA: 1:42 - loss: 4.814 - ETA: 1:41 - loss: 4.813 - ETA: 1:41 - loss: 4.808 - ETA: 1:40 - loss: 4.808 - ETA: 1:40 - loss: 4.801 - ETA: 1:40 - loss: 4.796 - ETA: 1:39 - loss: 4.794 - ETA: 1:39 - loss: 4.791 - ETA: 1:38 - loss: 4.788 - ETA: 1:38 - loss: 4.785 - ETA: 1:37 - loss: 4.784 - ETA: 1:37 - loss: 4.783 - ETA: 1:36 - loss: 4.784 - ETA: 1:36 - loss: 4.783 - ETA: 1:35 - loss: 4.779 - ETA: 1:35 - loss: 4.777 - ETA: 1:35 - loss: 4.774 - ETA: 1:34 - loss: 4.771 - ETA: 1:34 - loss: 4.770 - ETA: 1:33 - loss: 4.767 - ETA: 1:33 - loss: 4.763 - ETA: 1:32 - loss: 4.759 - ETA: 1:32 - loss: 4.758 - ETA: 1:31 - loss: 4.756 - ETA: 1:31 - loss: 4.754 - ETA: 1:30 - loss: 4.751 - ETA: 1:30 - loss: 4.749 - ETA: 1:29 - loss: 4.750 - ETA: 1:29 - loss: 4.748 - ETA: 1:28 - loss: 4.744 - ETA: 1:28 - loss: 4.743 - ETA: 1:28 - loss: 4.740 - ETA: 1:27 - loss: 4.736 - ETA: 1:27 - loss: 4.735 - ETA: 1:26 - loss: 4.734 - ETA: 1:26 - loss: 4.734 - ETA: 1:25 - loss: 4.730 - ETA: 1:25 - loss: 4.728 - ETA: 1:24 - loss: 4.723 - ETA: 1:24 - loss: 4.722 - ETA: 1:23 - loss: 4.721 - ETA: 1:23 - loss: 4.719 - ETA: 1:22 - loss: 4.717 - ETA: 1:22 - loss: 4.713 - ETA: 1:21 - loss: 4.713 - ETA: 1:21 - loss: 4.709 - ETA: 1:20 - loss: 4.705 - ETA: 1:20 - loss: 4.700 - ETA: 1:20 - loss: 4.698 - ETA: 1:19 - loss: 4.697 - ETA: 1:19 - loss: 4.694 - ETA: 1:18 - loss: 4.691 - ETA: 1:18 - loss: 4.686 - ETA: 1:17 - loss: 4.683 - ETA: 1:17 - loss: 4.681 - ETA: 1:16 - loss: 4.678 - ETA: 1:16 - loss: 4.679 - ETA: 1:15 - loss: 4.675 - ETA: 1:15 - loss: 4.670 - ETA: 1:14 - loss: 4.669 - ETA: 1:14 - loss: 4.668 - ETA: 1:13 - loss: 4.665 - ETA: 1:13 - loss: 4.665 - ETA: 1:12 - loss: 4.661 - ETA: 1:12 - loss: 4.659 - ETA: 1:12 - loss: 4.658 - ETA: 1:11 - loss: 4.656 - ETA: 1:11 - loss: 4.653 - ETA: 1:10 - loss: 4.651 - ETA: 1:10 - loss: 4.646 - ETA: 1:09 - loss: 4.644 - ETA: 1:09 - loss: 4.641 - ETA: 1:08 - loss: 4.637 - ETA: 1:08 - loss: 4.636 - ETA: 1:07 - loss: 4.634 - ETA: 1:07 - loss: 4.631 - ETA: 1:06 - loss: 4.628 - ETA: 1:06 - loss: 4.627 - ETA: 1:05 - loss: 4.623 - ETA: 1:05 - loss: 4.621 - ETA: 1:04 - loss: 4.620 - ETA: 1:04 - loss: 4.618 - ETA: 1:04 - loss: 4.617 - ETA: 1:03 - loss: 4.614 - ETA: 1:03 - loss: 4.611 - ETA: 1:02 - loss: 4.613 - ETA: 1:02 - loss: 4.615 - ETA: 1:01 - loss: 4.613 - ETA: 1:01 - loss: 4.613 - ETA: 1:00 - loss: 4.612 - ETA: 1:00 - loss: 4.613 - ETA: 59s - loss: 4.611 - ETA: 59s - loss: 4.60 - ETA: 58s - loss: 4.60 - ETA: 58s - loss: 4.60 - ETA: 57s - loss: 4.60 - ETA: 57s - loss: 4.59 - ETA: 56s - loss: 4.59 - ETA: 56s - loss: 4.59 - ETA: 55s - loss: 4.59 - ETA: 55s - loss: 4.58 - ETA: 55s - loss: 4.58 - ETA: 54s - loss: 4.58 - ETA: 54s - loss: 4.58 - ETA: 53s - loss: 4.58 - ETA: 53s - loss: 4.57 - ETA: 52s - loss: 4.57 - ETA: 52s - loss: 4.57 - ETA: 51s - loss: 4.57 - ETA: 51s - loss: 4.57 - ETA: 50s - loss: 4.56 - ETA: 50s - loss: 4.56 - ETA: 49s - loss: 4.56 - ETA: 49s - loss: 4.56 - ETA: 48s - loss: 4.55 - ETA: 48s - loss: 4.55 - ETA: 47s - loss: 4.55 - ETA: 47s - loss: 4.55 - ETA: 46s - loss: 4.55 - ETA: 46s - loss: 4.55 - ETA: 45s - loss: 4.55 - ETA: 45s - loss: 4.54 - ETA: 44s - loss: 4.54 - ETA: 44s - loss: 4.54 - ETA: 43s - loss: 4.54 - ETA: 43s - loss: 4.54 - ETA: 43s - loss: 4.53 - ETA: 42s - loss: 4.53 - ETA: 42s - loss: 4.53 - ETA: 41s - loss: 4.53 - ETA: 41s - loss: 4.53 - ETA: 40s - loss: 4.52 - ETA: 40s - loss: 4.52 - ETA: 39s - loss: 4.52 - ETA: 39s - loss: 4.51 - ETA: 38s - loss: 4.5169"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - ETA: 38s - loss: 4.51 - ETA: 37s - loss: 4.51 - ETA: 37s - loss: 4.51 - ETA: 36s - loss: 4.51 - ETA: 36s - loss: 4.51 - ETA: 35s - loss: 4.51 - ETA: 35s - loss: 4.50 - ETA: 34s - loss: 4.50 - ETA: 34s - loss: 4.50 - ETA: 34s - loss: 4.50 - ETA: 33s - loss: 4.50 - ETA: 33s - loss: 4.50 - ETA: 32s - loss: 4.49 - ETA: 32s - loss: 4.49 - ETA: 31s - loss: 4.49 - ETA: 31s - loss: 4.49 - ETA: 30s - loss: 4.49 - ETA: 30s - loss: 4.48 - ETA: 29s - loss: 4.48 - ETA: 29s - loss: 4.48 - ETA: 28s - loss: 4.48 - ETA: 28s - loss: 4.48 - ETA: 27s - loss: 4.48 - ETA: 27s - loss: 4.47 - ETA: 26s - loss: 4.47 - ETA: 26s - loss: 4.47 - ETA: 25s - loss: 4.47 - ETA: 25s - loss: 4.47 - ETA: 24s - loss: 4.46 - ETA: 24s - loss: 4.46 - ETA: 23s - loss: 4.46 - ETA: 23s - loss: 4.46 - ETA: 22s - loss: 4.46 - ETA: 22s - loss: 4.45 - ETA: 22s - loss: 4.45 - ETA: 21s - loss: 4.45 - ETA: 21s - loss: 4.45 - ETA: 20s - loss: 4.44 - ETA: 20s - loss: 4.44 - ETA: 19s - loss: 4.44 - ETA: 19s - loss: 4.44 - ETA: 18s - loss: 4.44 - ETA: 18s - loss: 4.44 - ETA: 17s - loss: 4.44 - ETA: 17s - loss: 4.44 - ETA: 16s - loss: 4.43 - ETA: 16s - loss: 4.43 - ETA: 15s - loss: 4.43 - ETA: 15s - loss: 4.43 - ETA: 14s - loss: 4.43 - ETA: 14s - loss: 4.43 - ETA: 13s - loss: 4.42 - ETA: 13s - loss: 4.42 - ETA: 12s - loss: 4.42 - ETA: 12s - loss: 4.42 - ETA: 11s - loss: 4.42 - ETA: 11s - loss: 4.42 - ETA: 10s - loss: 4.42 - ETA: 10s - loss: 4.42 - ETA: 10s - loss: 4.42 - ETA: 9s - loss: 4.4202 - ETA: 9s - loss: 4.418 - ETA: 8s - loss: 4.416 - ETA: 8s - loss: 4.414 - ETA: 7s - loss: 4.413 - ETA: 7s - loss: 4.411 - ETA: 6s - loss: 4.409 - ETA: 6s - loss: 4.409 - ETA: 5s - loss: 4.408 - ETA: 5s - loss: 4.407 - ETA: 4s - loss: 4.406 - ETA: 4s - loss: 4.403 - ETA: 3s - loss: 4.402 - ETA: 3s - loss: 4.401 - ETA: 2s - loss: 4.400 - ETA: 2s - loss: 4.398 - ETA: 1s - loss: 4.398 - ETA: 1s - loss: 4.397 - ETA: 0s - loss: 4.396 - ETA: 0s - loss: 4.393 - 190s 478ms/step - loss: 4.3933\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      ", , , 6683 , 3 , 2 , 1 , 0 , 0 , mentionhere mentionhere mentionhere mentionhere mentionhere mentionhere mentionhere you ' t\n",
      "\n",
      ", , , 3191 , 3 , 2 , 1 , 0 , 0 , mentionhere mentionhere mentionhere mentionhere mentionhere mentionhere mentionhere :\n",
      "\n",
      ", , , 18574 , 3 , 2 , 1 , 0 , 0 , mentionhere mentionhere mentionhere mentionhere mentionhere mentionhere mentionhere mentionhere :\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      ", , , 3121 , 3 , 2 , 1 , 0 , 0 , mentionhere mentionhere mentionhere mentionhere mentionhere faggot\n",
      "\n",
      ", , , 8812 , 3 , 2 , 1 , 0 , 0 , rt mentionhere : mentionhere mentionhere mentionhere will mentionhere faggot\n",
      "\n",
      ", , , 3191 , 3 , 2 , 1 , 0 , 0 , mentionhere mentionhere mentionhere mentionhere you ' t in a of fag\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "9063 16696 , 16074 , 3348 , 3 , 2 , 1 , 0 , 0 , mentionhere mentionhere i ' t beefing they you that 4394 dat blocking retarded . .\n",
      "\n",
      "22849 4221 , 11398 , 7201 , 3 , 2 , 1 , 0 , 0 , y ' t i to , know all spon to here dont y the ' t bitch in for boss colored \" \" \"\n",
      "\n",
      ", 8108 , heard , 3 , 2 , 1 , 0 , 0 , mentionhere mentionhere fucking a retarded\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/398 [=============>................] - ETA: 3:10 - loss: 3.276 - ETA: 3:23 - loss: 3.673 - ETA: 3:35 - loss: 3.633 - ETA: 3:33 - loss: 3.676 - ETA: 3:28 - loss: 3.591 - ETA: 3:25 - loss: 3.645 - ETA: 3:23 - loss: 3.665 - ETA: 3:21 - loss: 3.636 - ETA: 3:21 - loss: 3.608 - ETA: 3:19 - loss: 3.572 - ETA: 3:17 - loss: 3.573 - ETA: 3:16 - loss: 3.560 - ETA: 3:14 - loss: 3.594 - ETA: 3:13 - loss: 3.621 - ETA: 3:12 - loss: 3.622 - ETA: 3:11 - loss: 3.644 - ETA: 3:10 - loss: 3.667 - ETA: 3:09 - loss: 3.667 - ETA: 3:08 - loss: 3.680 - ETA: 3:07 - loss: 3.703 - ETA: 3:07 - loss: 3.702 - ETA: 3:06 - loss: 3.710 - ETA: 3:05 - loss: 3.710 - ETA: 3:04 - loss: 3.711 - ETA: 3:03 - loss: 3.703 - ETA: 3:02 - loss: 3.711 - ETA: 3:02 - loss: 3.721 - ETA: 3:00 - loss: 3.720 - ETA: 2:59 - loss: 3.718 - ETA: 2:58 - loss: 3.722 - ETA: 2:57 - loss: 3.719 - ETA: 2:56 - loss: 3.712 - ETA: 2:56 - loss: 3.693 - ETA: 2:55 - loss: 3.694 - ETA: 2:54 - loss: 3.687 - ETA: 2:53 - loss: 3.678 - ETA: 2:53 - loss: 3.690 - ETA: 2:52 - loss: 3.684 - ETA: 2:51 - loss: 3.680 - ETA: 2:51 - loss: 3.668 - ETA: 2:50 - loss: 3.661 - ETA: 2:49 - loss: 3.649 - ETA: 2:49 - loss: 3.637 - ETA: 2:48 - loss: 3.651 - ETA: 2:47 - loss: 3.659 - ETA: 2:46 - loss: 3.672 - ETA: 2:46 - loss: 3.660 - ETA: 2:45 - loss: 3.660 - ETA: 2:45 - loss: 3.664 - ETA: 2:44 - loss: 3.662 - ETA: 2:43 - loss: 3.662 - ETA: 2:43 - loss: 3.661 - ETA: 2:42 - loss: 3.663 - ETA: 2:41 - loss: 3.664 - ETA: 2:40 - loss: 3.665 - ETA: 2:40 - loss: 3.673 - ETA: 2:39 - loss: 3.670 - ETA: 2:38 - loss: 3.674 - ETA: 2:38 - loss: 3.668 - ETA: 2:37 - loss: 3.669 - ETA: 2:36 - loss: 3.660 - ETA: 2:36 - loss: 3.660 - ETA: 2:35 - loss: 3.656 - ETA: 2:34 - loss: 3.652 - ETA: 2:34 - loss: 3.651 - ETA: 2:33 - loss: 3.641 - ETA: 2:32 - loss: 3.637 - ETA: 2:32 - loss: 3.629 - ETA: 2:31 - loss: 3.629 - ETA: 2:30 - loss: 3.630 - ETA: 2:30 - loss: 3.629 - ETA: 2:29 - loss: 3.634 - ETA: 2:28 - loss: 3.637 - ETA: 2:28 - loss: 3.634 - ETA: 2:27 - loss: 3.636 - ETA: 2:27 - loss: 3.635 - ETA: 2:26 - loss: 3.635 - ETA: 2:25 - loss: 3.634 - ETA: 2:25 - loss: 3.631 - ETA: 2:24 - loss: 3.627 - ETA: 2:24 - loss: 3.627 - ETA: 2:23 - loss: 3.626 - ETA: 2:23 - loss: 3.630 - ETA: 2:22 - loss: 3.638 - ETA: 2:22 - loss: 3.643 - ETA: 2:22 - loss: 3.645 - ETA: 2:21 - loss: 3.651 - ETA: 2:21 - loss: 3.656 - ETA: 2:20 - loss: 3.658 - ETA: 2:20 - loss: 3.657 - ETA: 2:20 - loss: 3.657 - ETA: 2:19 - loss: 3.654 - ETA: 2:19 - loss: 3.653 - ETA: 2:19 - loss: 3.651 - ETA: 2:19 - loss: 3.646 - ETA: 2:18 - loss: 3.647 - ETA: 2:18 - loss: 3.651 - ETA: 2:17 - loss: 3.648 - ETA: 2:17 - loss: 3.644 - ETA: 2:16 - loss: 3.645 - ETA: 2:16 - loss: 3.643 - ETA: 2:15 - loss: 3.643 - ETA: 2:15 - loss: 3.641 - ETA: 2:15 - loss: 3.643 - ETA: 2:14 - loss: 3.643 - ETA: 2:14 - loss: 3.637 - ETA: 2:13 - loss: 3.637 - ETA: 2:13 - loss: 3.641 - ETA: 2:12 - loss: 3.642 - ETA: 2:12 - loss: 3.640 - ETA: 2:11 - loss: 3.636 - ETA: 2:11 - loss: 3.633 - ETA: 2:10 - loss: 3.632 - ETA: 2:10 - loss: 3.629 - ETA: 2:09 - loss: 3.632 - ETA: 2:08 - loss: 3.630 - ETA: 2:08 - loss: 3.627 - ETA: 2:07 - loss: 3.624 - ETA: 2:07 - loss: 3.622 - ETA: 2:06 - loss: 3.620 - ETA: 2:06 - loss: 3.618 - ETA: 2:05 - loss: 3.619 - ETA: 2:05 - loss: 3.622 - ETA: 2:04 - loss: 3.620 - ETA: 2:04 - loss: 3.618 - ETA: 2:03 - loss: 3.618 - ETA: 2:03 - loss: 3.619 - ETA: 2:02 - loss: 3.619 - ETA: 2:02 - loss: 3.619 - ETA: 2:01 - loss: 3.614 - ETA: 2:01 - loss: 3.611 - ETA: 2:00 - loss: 3.612 - ETA: 2:00 - loss: 3.612 - ETA: 1:59 - loss: 3.616 - ETA: 1:59 - loss: 3.620 - ETA: 1:58 - loss: 3.618 - ETA: 1:58 - loss: 3.618 - ETA: 1:57 - loss: 3.617 - ETA: 1:57 - loss: 3.617 - ETA: 1:56 - loss: 3.619 - ETA: 1:56 - loss: 3.625 - ETA: 1:55 - loss: 3.630 - ETA: 1:55 - loss: 3.630 - ETA: 1:55 - loss: 3.631 - ETA: 1:54 - loss: 3.628 - ETA: 1:54 - loss: 3.628 - ETA: 1:53 - loss: 3.628 - ETA: 1:53 - loss: 3.629 - ETA: 1:52 - loss: 3.626 - ETA: 1:52 - loss: 3.622 - ETA: 1:51 - loss: 3.619 - ETA: 1:51 - loss: 3.618 - ETA: 1:50 - loss: 3.622 - ETA: 1:50 - loss: 3.622 - ETA: 1:50 - loss: 3.621 - ETA: 1:49 - loss: 3.620 - ETA: 1:49 - loss: 3.618 - ETA: 1:49 - loss: 3.616 - ETA: 1:48 - loss: 3.613 - ETA: 1:48 - loss: 3.612 - ETA: 1:47 - loss: 3.612 - ETA: 1:47 - loss: 3.613 - ETA: 1:47 - loss: 3.613 - ETA: 1:46 - loss: 3.610 - ETA: 1:46 - loss: 3.608 - ETA: 1:45 - loss: 3.609 - ETA: 1:45 - loss: 3.609 - ETA: 1:44 - loss: 3.611 - ETA: 1:44 - loss: 3.609 - ETA: 1:43 - loss: 3.611 - ETA: 1:43 - loss: 3.610 - ETA: 1:42 - loss: 3.611 - ETA: 1:42 - loss: 3.609 - ETA: 1:42 - loss: 3.607 - ETA: 1:41 - loss: 3.604 - ETA: 1:41 - loss: 3.602 - ETA: 1:40 - loss: 3.605 - ETA: 1:40 - loss: 3.603 - ETA: 1:39 - loss: 3.600 - ETA: 1:39 - loss: 3.599 - ETA: 1:38 - loss: 3.597 - ETA: 1:38 - loss: 3.597 - ETA: 1:38 - loss: 3.594 - ETA: 1:37 - loss: 3.594 - ETA: 1:37 - loss: 3.593 - ETA: 1:36 - loss: 3.593 - ETA: 1:36 - loss: 3.596 - ETA: 1:35 - loss: 3.597 - ETA: 1:35 - loss: 3.595 - ETA: 1:34 - loss: 3.592 - ETA: 1:34 - loss: 3.5934"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-9fdf379812eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnew_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     word_level=True)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtextgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\textgenrnn\\textgenrnn.py\u001b[0m in \u001b[0;36mtrain_from_file\u001b[1;34m(self, file_path, header, delim, new_model, context, is_csv, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             self.train_new_model(\n\u001b[1;32m--> 327\u001b[1;33m                 texts, context_labels=context_labels, **kwargs)\n\u001b[0m\u001b[0;32m    328\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontext_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\textgenrnn\\textgenrnn.py\u001b[0m in \u001b[0;36mtrain_new_model\u001b[1;34m(self, texts, context_labels, num_epochs, gen_epochs, batch_size, dropout, train_size, validation, save_epochs, multi_gpu, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m                             \u001b[0msave_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                             \u001b[0mmulti_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m                             **kwargs)\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"textgenrnn_weights_saved.hdf5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\textgenrnn\\textgenrnn.py\u001b[0m in \u001b[0;36mtrain_on_texts\u001b[1;34m(self, texts, context_labels, batch_size, num_epochs, verbose, new_model, gen_epochs, train_size, max_gen_length, validation, dropout, via_new_model, save_epochs, multi_gpu, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m                               \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                               \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                               )\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train generative model \n",
    "textgen = textgenrnn(name='google')\n",
    "textgen.train_from_file(\n",
    "    file_path=class_path, \n",
    "    num_epochs=10, \n",
    "    batch_size=128, \n",
    "    new_model=True, \n",
    "    word_level=True)\n",
    "\n",
    "textgen.generate(5, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up temp file\n",
    "os.remove(class_path)\n",
    "os.remove('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
